

---

## SISTEMA DE MONITORAMENTO EM PRODU√á√ÉO (v2) üÜï

### Arquitetura de Monitoramento

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         API (FastAPI)                       ‚îÇ
‚îÇ  POST /predict ‚Üí Predi√ß√£o + Log             ‚îÇ
‚îÇ         ‚Üì                                    ‚îÇ
‚îÇ  monitoring/logs/prediction_log.csv          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Monitor (Semanal: Cron Job)              ‚îÇ
‚îÇ  python -m monitoring.monitor --window 7    ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  1. Carrega baseline (treino)               ‚îÇ
‚îÇ  2. Carrega logs (√∫ltimos 7 dias)           ‚îÇ
‚îÇ  3. Calcula PSI + KS Test                   ‚îÇ
‚îÇ  4. Detecta prediction drift                ‚îÇ
‚îÇ  5. Diagn√≥stico autom√°tico (se drift > 50%) ‚îÇ
‚îÇ  6. Gera relat√≥rio JSON                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Dashboard (HTML)                         ‚îÇ
‚îÇ  python generate_dashboard.py               ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚Ä¢ Status visual (‚úÖ‚ö†Ô∏èüö®)                   ‚îÇ
‚îÇ  ‚Ä¢ M√©tricas com limites                     ‚îÇ
‚îÇ  ‚Ä¢ Explica√ß√£o para leigos                   ‚îÇ
‚îÇ  ‚Ä¢ Recomenda√ß√µes autom√°ticas                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Features Est√°veis vs Dependentes de Escopo

**Problema do Modelo v1**:
- Features agregadas (`total_tx_cliente`, `tx_cliente_merchant`) dependem do tamanho do dataset
- Em produ√ß√£o, essas features t√™m valores diferentes do treino
- Causa: drift artificial (falsos positivos)

**Solu√ß√£o no Modelo v2**:
- Treinar apenas com **12 features est√°veis**
- Ignorar features que acumulam hist√≥rico

#### Features Est√°veis (v2)

```python
# features_config.py
FEATURES_ESTAVEIS = [
    # Features b√°sicas de entrada
    'age',                      # Faixa et√°ria (0-6)
    'gender_encoded',           # G√™nero codificado
    'category_encoded',         # Categoria da transa√ß√£o
    'amount',                   # Valor da transa√ß√£o
    
    # Features derivadas est√°veis
    'qtd_transacoes',          # Tx no mesmo step (n√£o acumula)
    'alert_freq',              # Alerta de frequ√™ncia (bin√°rio)
    'alert_valor',             # Alerta de valor (relativo)
    'valor_relativo_cliente',  # Valor / m√©dia do cliente
    
    # Features temporais com janela fixa
    'amount_media_5steps',     # M√©dia m√≥vel (janela = 5)
    
    # Features de relacionamento
    'primeira_tx_merchant',    # Primeira vez? (bin√°rio)
    
    # Features de localiza√ß√£o
    'mesma_localizacao',       # Mesma localiza√ß√£o? (bin√°rio)
    'num_zipcodes_cliente'     # Localiza√ß√µes distintas usadas
]
```

**Por qu√™ s√£o est√°veis?**:
- N√£o crescem indefinidamente com o hist√≥rico
- Calculadas sobre janelas fixas ou valores relativos
- Distribui√ß√£o consistente entre treino e produ√ß√£o

#### Features Exclu√≠das (Dependentes de Escopo)

```python
FEATURES_DEPENDENTES_ESCOPO = {
    'step',                    # Aumenta naturalmente com o tempo
    'total_tx_cliente',        # Cresce com hist√≥rico acumulado
    'volume_total_cliente',    # Cresce com hist√≥rico acumulado
    'num_categorias_cliente',  # Aumenta conforme uso
    'num_merchants_cliente',   # Aumenta conforme uso
    'amount_mean_cliente',     # Muda com hist√≥rico
    'amount_std_cliente',      # Muda com hist√≥rico
    'tx_cliente_merchant',     # Cresce com hist√≥rico
    'prop_tx_merchant',        # Derivada de total_tx_cliente
    'step_diff',               # Temporal, depende da janela
    'amount_desvio_5steps',    # Derivada de amount_media_5steps
    'tx_ultimos_5_steps'       # Temporal, menos cr√≠tica
}
```

---

### Pipeline de Treino com Baseline

```python
# train.py (v2)
def salvar_baseline_monitoring(X_train, y_train, model, output_path):
    """
    Salva estat√≠sticas de treino para compara√ß√£o em produ√ß√£o
    """
    baseline = {
        'created_at': datetime.now().isoformat(),
        'model_version': 'v2',
        'n_samples': len(X_train),
        'features': {},
        'target': {
            'fraud_rate': float(y_train.mean())
        },
        'predictions': {}
    }
    
    # Estat√≠sticas de cada feature (apenas est√°veis)
    features_monitoradas = [
        col for col in X_train.columns 
        if col not in FEATURES_DEPENDENTES_ESCOPO
    ]
    
    for col in features_monitoradas:
        baseline['features'][col] = {
            'mean': float(X_train[col].mean()),
            'std': float(X_train[col].std()),
            'q25': float(X_train[col].quantile(0.25)),
            'q75': float(X_train[col].quantile(0.75))
        }
    
    # Distribui√ß√£o das predi√ß√µes no treino
    y_pred_proba = model.predict_proba(X_train)[:, 1]
    baseline['predictions'] = {
        'mean_proba': float(y_pred_proba.mean()),
        'std_proba': float(y_pred_proba.std()),
        'q10': float(np.percentile(y_pred_proba, 10)),
        'q50': float(np.percentile(y_pred_proba, 50)),
        'q90': float(np.percentile(y_pred_proba, 90)),
        'fraud_pred_rate': float((y_pred_proba >= 0.5).mean())
    }
    
    with open(output_path, 'w') as f:
        json.dump(baseline, f, indent=2)
    
    return baseline

# No final do treino:
salvar_baseline_monitoring(X_train, y_train, model)
```

**Artefatos Gerados**:
```
artifacts/model_v2/
‚îú‚îÄ‚îÄ model.pkl                    # Modelo treinado
‚îî‚îÄ‚îÄ reference_features_v2.csv    # Features do treino (12 colunas)

monitoring/baseline/
‚îî‚îÄ‚îÄ baseline_stats.json          # Estat√≠sticas de refer√™ncia
```

---

### API com Logging Autom√°tico

```python
# main.py (FastAPI)
@app.post("/predict")
def predict(transaction: Transaction):
    start_time = time.time()
    request_id = str(uuid.uuid4())
    
    # Construir features (12 features est√°veis)
    df = pd.DataFrame([transaction.dict()])
    X, _ = build_features(df)
    
    # Predi√ß√£o
    proba = model.predict_proba(X)[0, 1]
    pred = int(proba >= 0.5)
    
    latency_ms = round((time.time() - start_time) * 1000, 2)
    
    # üîπ Log para monitoramento (autom√°tico)
    log_prediction(X, pred, proba, request_id, latency_ms)
    
    return {
        "request_id": request_id,
        "fraud_probability": float(proba),
        "fraud_prediction": pred,
        "model_version": "v2",
        "latency_ms": latency_ms
    }

def log_prediction(X_input, prediction, proba, request_id, latency_ms):
    """
    Registra todas as features + predi√ß√£o + metadados
    """
    log_df = X_input.copy()
    
    log_df["prediction"] = int(prediction)
    log_df["probability"] = float(proba)
    log_df["model_version"] = "v2"
    log_df["request_id"] = request_id
    log_df["latency_ms"] = latency_ms
    log_df["timestamp"] = datetime.now()
    
    # Append ao CSV
    file_exists = os.path.isfile(LOG_PATH)
    log_df.to_csv(LOG_PATH, mode="a", header=not file_exists, index=False)
```

**prediction_log.csv**:
```csv
age,gender_encoded,category_encoded,amount,qtd_transacoes,...,prediction,probability,model_version,request_id,latency_ms,timestamp
3,1,12,150.00,1,...,0,0.0234,v2,550e8400...,25.3,2026-02-17 10:23:45
```

---

### Monitor - Data Drift Detection

#### PSI (Population Stability Index)

```python
def calcular_psi(baseline_array, production_array, bins=10):
    """
    PSI mede o quanto a distribui√ß√£o mudou
    
    F√≥rmula:
    PSI = Œ£ (P_prod - P_base) * ln(P_prod / P_base)
    
    Interpreta√ß√£o:
    - PSI < 0.1  : Sem mudan√ßa significativa ‚úÖ
    - PSI 0.1-0.25: Mudan√ßa moderada ‚ö†Ô∏è
    - PSI > 0.25 : Mudan√ßa significativa üö®
    """
    # Criar bins baseados na baseline
    percentiles = np.linspace(0, 100, bins + 1)
    bin_edges = np.percentile(baseline_array, percentiles)
    bin_edges = np.unique(bin_edges)
    
    # Calcular distribui√ß√µes
    baseline_dist, _ = np.histogram(baseline_array, bins=bin_edges)
    production_dist, _ = np.histogram(production_array, bins=bin_edges)
    
    # Normalizar
    baseline_dist = baseline_dist / len(baseline_array)
    production_dist = production_dist / len(production_array)
    
    # Evitar log(0)
    baseline_dist = np.where(baseline_dist == 0, 0.0001, baseline_dist)
    production_dist = np.where(production_dist == 0, 0.0001, production_dist)
    
    # Calcular PSI
    psi = np.sum((production_dist - baseline_dist) * 
                 np.log(production_dist / baseline_dist))
    
    return float(psi)
```

**Exemplo de Drift**:
```python
# Feature: amount
baseline_values = [100, 120, 110, 105, ...]  # Treino
production_values = [500, 600, 550, 580, ...] # Produ√ß√£o (6 meses depois)

psi = calcular_psi(baseline_values, production_values)
# PSI = 1.234 ‚Üí üö® Drift cr√≠tico! Valores muito maiores
```

#### KS Test (Kolmogorov-Smirnov)

```python
from scipy import stats

def test_ks(baseline_values, production_values):
    """
    KS test: Testa se duas distribui√ß√µes s√£o iguais
    
    H0: Distribui√ß√µes s√£o iguais
    H1: Distribui√ß√µes s√£o diferentes
    
    Se p-value < 0.05 ‚Üí Rejeita H0 ‚Üí Drift detectado
    """
    ks_stat, p_value = stats.ks_2samp(baseline_values, production_values)
    
    return {
        'ks_statistic': float(ks_stat),
        'p_value': float(p_value),
        'drift': p_value < 0.05
    }
```

---

### Monitor - Prediction Drift Detection

```python
def monitorar_prediction_drift(df_prod, baseline):
    """
    Detecta mudan√ßas nas predi√ß√µes do modelo
    """
    proba_prod = df_prod['probability'].values
    base_pred = baseline['predictions']
    
    mean_proba = proba_prod.mean()
    fraud_rate = (proba_prod >= 0.5).mean()
    
    mean_change = abs(mean_proba - base_pred['mean_proba']) / base_pred['mean_proba'] * 100
    
    print(f"Prob M√©dia: {mean_proba:.4f} (baseline: {base_pred['mean_proba']:.4f})")
    print(f"Mudan√ßa: {mean_change:+.1f}%")
    
    # Thresholds
    if mean_change > 50:
        print("üö® ALERTA CR√çTICO: Rodar diagn√≥stico")
        diagnosticar_prediction_drift(df_prod, baseline)
    elif mean_change > 20:
        print("‚ö†Ô∏è  ATEN√á√ÉO: Monitorar de perto")
    else:
        print("‚úÖ Predi√ß√µes est√°veis")
```

**Exemplo de Output**:
```
üéØ PREDICTION DRIFT
============================================================
Prob M√©dia : 0.0171  (baseline: 0.0121) | Œî +41.2%
Taxa Fraude: 0.0097  (baseline: 0.0097) | Œî +0.3%
‚ö†Ô∏è  ATEN√á√ÉO: M√©dia mudou 41.2%
```

---

### Diagn√≥stico Autom√°tico de Drift

Quando drift > 50%, o sistema investiga automaticamente:

```python
def diagnosticar_prediction_drift(df_prod, baseline):
    """
    Investiga as causas raiz do drift
    """
    # 1. Segmenta√ß√£o por faixas de risco
    print("\nüéØ Segmenta√ß√£o por Faixas de Risco:")
    faixas = {
        'Baixo risco (< 0.1)': (df_prod['probability'] < 0.1).sum(),
        'Risco moderado (0.1-0.3)': ((df_prod['probability'] >= 0.1) & 
                                     (df_prod['probability'] < 0.3)).sum(),
        'Risco alto (0.3-0.7)': ((df_prod['probability'] >= 0.3) & 
                                  (df_prod['probability'] < 0.7)).sum(),
        'Risco cr√≠tico (‚â• 0.7)': (df_prod['probability'] >= 0.7).sum()
    }
    
    for faixa, count in faixas.items():
        pct = count / len(df_prod) * 100
        print(f"   {faixa:30s}: {count:5d} ({pct:5.1f}%)")
    
    # 2. Identificar features suspeitas
    print("\nüîç Features com Maior Impacto:")
    features_suspeitas = []
    
    for feature in df_prod.columns[:10]:  # Top 10 features
        if feature in ['probability', 'prediction', 'latency_ms']:
            continue
        
        # Dividir em quartis
        q75 = df_prod[feature].quantile(0.75)
        
        # Comparar prob m√©dia entre Q4 e Q1-3
        proba_q4 = df_prod[df_prod[feature] >= q75]['probability'].mean()
        proba_q1_3 = df_prod[df_prod[feature] < q75]['probability'].mean()
        
        diff = abs(proba_q4 - proba_q1_3)
        
        if diff > 0.05:  # Diferen√ßa significativa
            features_suspeitas.append({
                'feature': feature,
                'diff': diff,
                'proba_q4': proba_q4,
                'proba_q1_3': proba_q1_3
            })
    
    # Ordenar por impacto
    features_suspeitas.sort(key=lambda x: x['diff'], reverse=True)
    
    for item in features_suspeitas[:5]:
        print(f"   ‚Ä¢ {item['feature']:25s}: Q4 prob={item['proba_q4']:.4f} vs Q1-3 prob={item['proba_q1_3']:.4f} (Œî {item['diff']:.4f})")
    
    # 3. An√°lise temporal
    print("\nüìÖ An√°lise Temporal:")
    df_prod['dia'] = pd.to_datetime(df_prod['timestamp']).dt.date
    proba_por_dia = df_prod.groupby('dia')['probability'].mean()
    
    primeira_metade = proba_por_dia.iloc[:len(proba_por_dia)//2].mean()
    segunda_metade = proba_por_dia.iloc[len(proba_por_dia)//2:].mean()
    
    mudanca_temporal = ((segunda_metade - primeira_metade) / primeira_metade) * 100
    
    print(f"   1¬™ metade: {primeira_metade:.4f}")
    print(f"   2¬™ metade: {segunda_metade:.4f}")
    
    if abs(mudanca_temporal) > 10:
        print(f"   ‚ö†Ô∏è  Tend√™ncia temporal: {mudanca_temporal:+.1f}%")
    else:
        print("   ‚úÖ Probabilidades est√°veis ao longo do tempo")
    
    # 4. Conclus√µes
    print("\nüí° Conclus√µes:")
    mean_proba = df_prod['probability'].mean()
    baseline_mean = baseline['predictions']['mean_proba']
    
    if mean_proba > baseline_mean * 3:
        print("   üö® Drift severo detectado (>300%)")
        print("   Recomenda√ß√£o: Retreinar modelo imediatamente")
    elif mean_proba > baseline_mean * 1.5:
        print("   ‚ö†Ô∏è  Drift moderado detectado (>150%)")
        print("   Recomenda√ß√£o: Monitorar e retreinar em 30 dias")
    else:
        print("   ‚úÖ Drift dentro do esperado")
```

**Exemplo de Output**:
```
üî¨ DIAGN√ìSTICO DE DRIFT - Investigando Causas
============================================================

üìä Distribui√ß√£o das Probabilidades:
   Baseline | P50: 0.0000 | P90: 0.0011
   Produ√ß√£o | P50: 0.0783 | P90: 0.1271

üéØ Segmenta√ß√£o por Faixas de Risco:
   Baixo risco (< 0.1)           :  7849 ( 78.5%)
   Risco moderado (0.1-0.3)      :  2024 ( 20.3%)
   Risco alto (0.3-0.7)          :    66 (  0.7%)
   Risco cr√≠tico (‚â• 0.7)         :    55 (  0.6%)

üîç Features com Maior Impacto:
   ‚Ä¢ category_encoded         : Q4 prob=0.0855 vs Q1-3 prob=0.1387 (Œî 0.0531)
   ‚Ä¢ amount                   : Q4 prob=0.1234 vs Q1-3 prob=0.0567 (Œî 0.0667)

üìÖ An√°lise Temporal:
   1¬™ metade: 0.0915
   2¬™ metade: 0.0914
   ‚úÖ Probabilidades est√°veis ao longo do tempo

üí° Conclus√µes:
   üö® Drift severo detectado (>300%)
   Poss√≠veis causas:
      ‚Ä¢ Popula√ß√£o de clientes mudou (mais clientes veteranos)
      ‚Ä¢ Concentra√ß√£o de transa√ß√µes de alto risco
   Recomenda√ß√£o: Investigar features_suspeitas e considerar retreinamento
```

---

### Dashboard HTML Visual

```python
# generate_dashboard.py
def gerar_dashboard_html(report_path):
    """
    Gera dashboard HTML interativo a partir do relat√≥rio JSON
    """
    with open(report_path, 'r') as f:
        report = json.load(f)
    
    # Determinar status geral
    critical_alerts = report['summary']['critical_alerts']
    warnings = report['summary']['warnings']
    
    if critical_alerts > 0:
        status = "üö® CR√çTICO"
        status_color = "#dc3545"  # Vermelho
    elif warnings > 3:
        status = "‚ö†Ô∏è ATEN√á√ÉO"
        status_color = "#fd7e14"  # Laranja
    else:
        status = "‚úÖ SAUD√ÅVEL"
        status_color = "#28a745"  # Verde
    
    # Gerar HTML com:
    # - Cards de m√©tricas
    # - Tabela de features com PSI e limites
    # - Explica√ß√£o em portugu√™s claro
    # - Recomenda√ß√µes autom√°ticas
    
    return html
```

**Elementos do Dashboard**:
1. **Status Visual Grande**: `‚úÖ SAUD√ÅVEL` | `‚ö†Ô∏è ATEN√á√ÉO` | `üö® CR√çTICO`
2. **Cards com M√©tricas**:
   - Alertas cr√≠ticos / avisos
   - Probabilidade m√©dia + limite (< 50%)
   - Lat√™ncia + limite (< 500ms)
3. **Tabela de Features**:
   - Status (‚úÖ‚ö†Ô∏èüö®)
   - Nome da feature
   - PSI + limite
   - Mudan√ßa %
4. **Explica√ß√£o para Leigos**:
   - "O modelo est√° funcionando perfeitamente!"
   - "De cada 1000 transa√ß√µes, o modelo prev√™ X fraudes"
   - "O modelo responde em X ms (super r√°pido!)"
5. **Pr√≥ximos Passos**:
   - ‚úÖ Continue monitorando
   - ‚ö†Ô∏è Agende revis√£o
   - üö® Retreine imediatamente

---

### Simulador de Produ√ß√£o

```python
# simulate_requests.py
def simulate():
    """
    Simula dados de produ√ß√£o com timestamps distribu√≠dos
    """
    df = pd.read_csv("data/raw/v2/bs140513_032310_v2.csv")
    
    # Amostra aleat√≥ria (distribui√ß√£o representativa)
    df_sample = df.sample(n=10000, random_state=42)
    
    # Construir features
    X, _ = build_features(df_sample)
    
    # Carregar modelo
    model = joblib.load("artifacts/model_v2/model.pkl")
    
    # Gerar timestamps distribu√≠dos nos √∫ltimos 7 dias
    timestamps = gerar_timestamps_distribuidos(10000, dias=7)
    
    # Predi√ß√µes + log
    for i in range(len(X)):
        row = X.iloc[[i]]
        proba = model.predict_proba(row)[0, 1]
        pred = int(proba >= 0.5)
        
        log_prediction(row, pred, proba, uuid.uuid4(), 26.0, timestamps[i])
    
    print(f"‚úÖ {10000} registros simulados")

def gerar_timestamps_distribuidos(n, dias=7):
    """
    Gera timestamps uniformemente distribu√≠dos nos √∫ltimos N dias
    """
    agora = datetime.now()
    inicio = agora - timedelta(days=dias)
    
    segundos_totais = int(timedelta(days=dias).total_seconds())
    offsets = np.sort(np.random.randint(0, segundos_totais, size=n))
    
    timestamps = [inicio + timedelta(seconds=int(s)) for s in offsets]
    return timestamps
```

---

### Workflow Completo de Monitoramento

```bash
# 1. Treinar modelo (gera baseline)
python -m src.models.v2.train
# Output:
#   - artifacts/model_v2/model.pkl
#   - artifacts/model_v2/reference_features_v2.csv
#   - monitoring/baseline/baseline_stats.json

# 2. Rodar API (coleta logs)
uvicorn src.api.main:app --reload
# Logs salvos em: monitoring/logs/prediction_log.csv

# 3. Simular produ√ß√£o (opcional - para teste)
python simulate_requests.py
# Gera 10k predi√ß√µes simuladas nos √∫ltimos 7 dias

# 4. Monitorar (semanal - cron job)
python -m monitoring.monitor --window 7
# Output:
#   - Relat√≥rio no terminal
#   - monitoring/reports/drift_report_YYYYMMDD_HHMMSS.json

# 5. Gerar dashboard (visualiza√ß√£o)
python generate_dashboard.py
# Output: monitoring/reports/dashboard.html
```

---

### Thresholds e Alertas

| M√©trica | Threshold | A√ß√£o |
|---------|-----------|------|
| **PSI** | < 0.1 | ‚úÖ Normal |
| | 0.1 - 0.25 | ‚ö†Ô∏è Monitorar |
| | > 0.25 | üö® Retreinar |
| **Prediction Drift** | < 20% | ‚úÖ Normal |
| | 20-50% | ‚ö†Ô∏è Investigar |
| | > 50% | üö® Diagn√≥stico autom√°tico |
| **Lat√™ncia** | < 100ms | ‚úÖ Excelente |
| | 100-500ms | ‚ö†Ô∏è Aceit√°vel |
| | > 500ms | üö® Problema de performance |
| **KS p-value** | > 0.05 | ‚úÖ Sem drift |
| | < 0.05 | üö® Drift detectado |

---

### Estrutura de Arquivos Gerados

```
monitoring/
‚îú‚îÄ‚îÄ monitor.py                  # Script principal
‚îú‚îÄ‚îÄ baseline/
‚îÇ   ‚îî‚îÄ‚îÄ baseline_stats.json     # Estat√≠sticas de treino
‚îÇ       {
‚îÇ         "created_at": "2026-02-17T17:14:48",
‚îÇ         "model_version": "v2",
‚îÇ         "n_samples": 475714,
‚îÇ         "features": {
‚îÇ           "age": {"mean": 3.0, "std": 1.34, "q25": 2.0, "q75": 4.0},
‚îÇ           ...
‚îÇ         },
‚îÇ         "target": {"fraud_rate": 0.0121},
‚îÇ         "predictions": {
‚îÇ           "mean_proba": 0.0121,
‚îÇ           "q50": 0.0000,
‚îÇ           "q90": 0.0011
‚îÇ         }
‚îÇ       }
‚îÇ
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ prediction_log.csv      # Logs de produ√ß√£o
‚îÇ       age,gender_encoded,amount,...,prediction,probability,timestamp
‚îÇ       3,1,150.00,...,0,0.0234,2026-02-17 10:23:45
‚îÇ
‚îî‚îÄ‚îÄ reports/
    ‚îú‚îÄ‚îÄ drift_report_20260217_105615.json  # Relat√≥rio JSON
    ‚îÇ   {
    ‚îÇ     "timestamp": "2026-02-17T10:56:15",
    ‚îÇ     "window_days": 7,
    ‚îÇ     "data_drift": {
    ‚îÇ       "features": {"age": {"psi": 0.000, ...}},
    ‚îÇ       "alertas": []
    ‚îÇ     },
    ‚îÇ     "prediction_drift": {...},
    ‚îÇ     "operational_performance": {...},
    ‚îÇ     "summary": {"critical_alerts": 0, "warnings": 1}
    ‚îÇ   }
    ‚îÇ
    ‚îî‚îÄ‚îÄ dashboard.html          # Dashboard visual
```

---

## TAGS DE BUSCA

`#mlops` `#mlflow` `#fraud-detection` `#feature-engineering` `#random-forest` `#docker` `#fastapi` `#model-monitoring` `#drift-detection` `#psi` `#scikit-learn` `#banksim` `#production-ml`

---

**Vers√£o**: 2.0 üÜï  
**Compatibilidade**: Python 3.9+, Scikit-learn 1.0+, MLflow 2.0+, FastAPI 0.100+  
**Uso recomendado**: MLOps, produ√ß√£o, monitoramento cont√≠nuo, drift detection
**√öltima atualiza√ß√£o**: Fevereiro 2026 - Sistema completo de monitoramento implementado
